集成算法
==========================================================
集成算法思路
  -1. 如果单个模型由于比较容易受噪音数据的影响，所以导致构建的模型效果不稳定，可以考虑将多个不太稳定的模型的预测结果做一个合并，从而降低这个不稳定性 --> Bagging
  -2. 如果单个模型的预测能力较弱(效果至少大于0.5), 可以考虑一下串行的方式来构建多个弱学习器，让后面的学习器在前面学习器的基础上来构建，从而让整个模型的效果变好 --> Boosting
  NOTE：集成学习就是将多个模型合并到一起，然后让模型效果更稳当、更好。
==========================================================
Bagging
  思想：基于有放回的抽样产生不同的数据集子集(样本不一样、样本数量不一样)，然后使用不同的数据子集训练不同的模型(模型类型一样、但是模型学习到的特征不一样)，最终将多个不同的模型预测结果合并
  代表算法：随机森林
==========================================================  
Boosting
  思想：让第m个模型的构建，是在前m-1个模型的预测基础上来构建的，让模型的构建过程是向着让所有训练样本的预测值和实际值差值/误差最小化的方向来更新的。
  代表算法：
    Adaboost：
	  通过修改样本的权重，在第M个子模型构建的时候，会使用前一个模型来修改样本的权重，如果样本在前一个模型中预测失败，那么样本权重加大，如果预测成功，样本权重减小；增大样本权重可以保证在当前子模型的训练过程中，对于权重大的样本着重考虑，尽可能的预测成功。
	  NOTE：
	    两个权重：
		  -1. 样本权重：如果在之前模型中，样本预测失败，那么该样本的权重增大，否则减小。
		  -2. 子模型的权重：相当于是子模型的可行度，子模型的误差越小，那么权重越大，否则越小。
	GBDT：
	  第m个模型的构建，会基于前m-1个模型的结果来构建；让第m个模型的构成过程是基于之前模型的损失函数的负梯度方向来进行变化的，变化的主要目的是让损失函数的值最小化，所以说每一个子模型都是在之前模型的预测结果基础上让预测的误差更新(损失更新)
	  NOTE：
	    GBDT通过更新训练数据的目标属性y值让模型效果更好
	
==========================================================
Stacking
  思想：分为两个阶段，第一阶段主要负责基于原始数据构建不同的子模型，第二阶段主要使用第一阶段的模型的输出预测值作为特征属性来构建最终的子模型
  eg：
    原始数据x1,x2,x3,y
    第一阶段：
		1. 模型1：
		  使用x1,x2,x3作为特征属性，y作为目标属性，训练一个子模型m1
		  将训练数据输入到m1中，得到预测值h1
		2. 模型2：
		  使用x1,x2,x3作为特征属性，y作为目标属性，训练一个子模型m2
		  将训练数据输入到m2中，得到预测值h2
		3. 模型3：
		  使用x1,x2,x3作为特征属性，y作为目标属性，训练一个子模型m3
		  将训练数据输入到m3中，得到预测值h3
		4. 模型4：
		  使用x1,x2,x3作为特征属性，y作为目标属性，训练一个子模型m4
		  将训练数据输入到m4中，得到预测值h4
	第二阶段：
	    使用第一阶段的输出预测值h1\h2\h3\h4作为特征属性，y作为目标属性，训练一个最终的子模型m
   预测：
     1. 将样本x输入到m1\m2\m3\m4分别得到预测值h1\h2\h3\h4，然后将其组成一个向量输入到模型m中，得到最终的预测值y
==========================================================
XGBoost
  相比于GBDT而言
    -1. 内部使用的是损失函数的二阶导函数，所以执行速度以及效果会比GBDT快/好
	-2. 内部嵌入了正则化项结构，相比于GBDT更加不容易过拟合
	-3. XGBoost可以在特征粒度上进行并行化选择最优划分，GBDT在实现的时候，是串行的最优划分的。
	
	
	
下周内容：
  周六：聚类算法
  周天：SVM
  周二晚上(20:00~21:00)：答疑+作业(波士顿租赁回归代码实现)
  周四晚上(20:00~21:00)：答疑+KNN伪代码回顾一遍（把伪代码换成可以运行的代码）
  
	
	
	
	